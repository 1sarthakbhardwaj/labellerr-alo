# üîÑ Labellerr SDK vs ALO - Why We Built This

## The Problem with Direct SDK Usage

The **Labellerr SDK** is powerful but requires significant boilerplate and manual orchestration:

### ‚ùå **Messy SDK Code Example**

```python
import time
import os
from labellerr.client import LabellerrClient
from labellerr.exceptions import LabellerrError

# Initialize client
client = LabellerrClient(api_key, api_secret)

# Create project - complex nested payload
project_payload = {
    'client_id': '12345',
    'dataset_name': 'My Dataset',
    'dataset_description': 'A sample dataset',
    'data_type': 'image',
    'created_by': 'user@example.com',
    'project_name': 'My Project',
    'annotation_guide': [
        {
            "question_number": 1,
            "question": "What is in the image?",
            "question_id": "uuid-here",
            "option_type": "dropdown",
            "required": True,
            "options": [
                {"option_name": "Cat"},
                {"option_name": "Dog"},
                {"option_name": "Car"}
            ]
        }
    ],
    'rotation_config': {
        'annotation_rotation_count': 1,
        'review_rotation_count': 1,
        'client_review_rotation_count': 0
    },
    'autolabel': False,
    'folder_to_upload': '/path/to/images'
}

try:
    # Create project
    result = client.initiate_create_project(project_payload)
    project_id = result['project_id']
    print(f"Project created: {project_id}")
except LabellerrError as e:
    print(f"Error: {str(e)}")
    # Manual error handling...

# Upload files
try:
    files_result = client.upload_files('12345', ['/path/file1.jpg', '/path/file2.jpg'])
    connection_id = files_result['connection_id']
except LabellerrError as e:
    print(f"Upload error: {str(e)}")

# Wait for dataset to be ready (manual polling)
while True:
    try:
        dataset = client.get_dataset('12345', dataset_id)
        if dataset['status'] == 'ready':
            break
    except:
        pass
    time.sleep(10)

# Run your model predictions (separate code)
# ... your model code here ...

# Upload pre-annotations
try:
    result = client.upload_preannotation_by_project_id(
        project_id, '12345', 'coco', 'predictions.json'
    )
except LabellerrError as e:
    print(f"Pre-annotation error: {str(e)}")

# Monitor status (more manual polling)
while True:
    status = client.preannotation_job_status()
    if status['response']['status'] == 'completed':
        break
    time.sleep(30)

# Export results
export_config = {
    "export_name": "My Export",
    "export_description": "Export of annotations",
    "export_format": "json",
    "statuses": ["accepted"]
}

try:
    export = client.create_local_export(project_id, '12345', export_config)
    print(f"Export created: {export['export_id']}")
except LabellerrError as e:
    print(f"Export error: {str(e)}")
```

### üò´ **Problems:**
- **50+ lines** of boilerplate code
- **Complex nested dictionaries** for configuration
- **Manual polling** for status checks
- **Repetitive error handling** everywhere
- **No orchestration** of multi-step workflows
- **Hard to maintain** - changes require code modifications
- **Not reusable** - different for each project

---

## ‚úÖ The ALO Solution

### **Clean, Declarative, Powerful**

```yaml
# workflow.yaml - 30 lines, zero boilerplate!
name: "my_labeling_pipeline"
description: "Complete labeling workflow in one config"

parameters:
  project_id: "your_project_id"
  dataset_path: "/path/to/images"

steps:
  - name: "discover_classes"
    agent: "object_discoverer"
    parameters:
      dataset_path: "${dataset_path}"
      sample_percentage: 0.05
      min_samples: 2
  
  - name: "label_dataset"
    agent: "production_labeler"
    parameters:
      classes: "${discover_classes.discovered_classes}"
      dataset_path: "${dataset_path}"
    depends_on: ["discover_classes"]
  
  - name: "validate"
    agent: "llm_validator"
    parameters:
      min_confidence: 0.7
    depends_on: ["label_dataset"]
  
  - name: "export"
    action: "push_to_labellerr"
    parameters:
      project_id: "${project_id}"
      format: "coco_json"
    depends_on: ["validate"]
```

```python
# Run it - 3 lines!
from alo import WorkflowOrchestrator
from alo.connectors import LabellerrConnector

connector = LabellerrConnector(api_key, api_secret, client_id)
orchestrator = WorkflowOrchestrator("workflow.yaml")
results = orchestrator.run(connector)
```

### üéâ **Benefits:**
- **3 lines** of Python code (vs 50+)
- **Declarative YAML** config (vs complex nested dicts)
- **Automatic polling** and status management
- **Built-in error handling** and retries
- **Workflow orchestration** out of the box
- **Reusable** across projects
- **Maintainable** - change YAML, not code

---

## üöÄ What ALO Adds to Labellerr SDK

| Feature | SDK Alone | ALO |
|---------|-----------|-----|
| **Project Creation** | Manual payload building | YAML config |
| **File Upload** | Manual batching | Automatic batching |
| **Status Monitoring** | Manual polling loops | Automatic |
| **Error Handling** | Try-except everywhere | Built-in |
| **Workflow Steps** | Sequential Python code | Declarative YAML |
| **Data Flow** | Manual variable passing | Automatic |
| **Model Integration** | Separate scripts | Built-in agents |
| **Validation** | Custom code | AI agents |
| **Active Learning** | Custom implementation | Built-in |
| **Reusability** | Copy-paste code | Reuse YAML |
| **Class Discovery** | Manual listing | Automatic |
| **Multi-Model** | Complex integration | Simple config |

---

## üìä Real-World Comparison

### **Scenario**: Label 10,000 pet images with quality validation

#### **Using SDK Directly**

```python
# File: pet_labeling_sdk.py (200+ lines)

import time
from labellerr.client import LabellerrClient

client = LabellerrClient(api_key, api_secret)

# 1. Create project (20 lines of payload)
project_payload = {...}  # Complex nested structure
result = client.initiate_create_project(project_payload)

# 2. Upload images (10 lines with error handling)
try:
    files = [...]  # List all files
    client.upload_files(client_id, files)
except:
    # Handle errors
    pass

# 3. Wait for upload (5 lines of polling)
while True:
    # Check status
    time.sleep(10)

# 4. Run YOLO predictions (50 lines of custom code)
from ultralytics import YOLO
model = YOLO('yolov8x.pt')
predictions = []
for image in images:
    pred = model(image)
    predictions.append(pred)

# 5. Format as COCO (30 lines of formatting)
coco_data = format_predictions(predictions)

# 6. Upload pre-annotations (10 lines)
client.upload_preannotation_by_project_id(...)

# 7. Monitor upload (5 lines of polling)
while True:
    status = client.preannotation_job_status()
    time.sleep(30)

# 8. Validate predictions (50 lines of custom validation)
validated_preds = validate_with_llm(predictions)

# 9. Export results (15 lines)
export_config = {...}
client.create_local_export(...)

# Total: 200+ lines, many failure points, hard to maintain
```

**Time to implement**: 2-4 hours  
**Lines of code**: 200+  
**Error-prone**: High  
**Reusable**: No

#### **Using ALO**

```yaml
# File: pet_labeling_alo.yaml (40 lines)

name: "pet_labeling"
steps:
  - name: "discover_pets"
    agent: "object_discoverer"
    parameters:
      dataset_path: "/path/to/10000/pets"
      sample_percentage: 0.05
  
  - name: "detect_all"
    agent: "yolo_detector"
    parameters:
      classes: "${discover_pets.discovered_classes}"
      dataset_path: "/path/to/10000/pets"
    depends_on: ["discover_pets"]
  
  - name: "validate"
    agent: "llm_validator"
    parameters:
      min_confidence: 0.7
    depends_on: ["detect_all"]
  
  - name: "export"
    action: "push_to_labellerr"
    parameters:
      project_id: "${project_id}"
    depends_on: ["validate"]
```

```python
# File: run_pet_labeling.py (3 lines)
orchestrator = WorkflowOrchestrator("pet_labeling_alo.yaml")
results = orchestrator.run(connector)
print(f"Discovered: {results['discover_pets']['discovered_classes']}")
```

**Time to implement**: 10 minutes  
**Lines of code**: 43 (YAML + Python)  
**Error-prone**: Low  
**Reusable**: Yes

---

## üéØ Why ALO is an Open Source Initiative

### **Making AI Labeling Accessible**

ALO isn't just a wrapper around the SDK - it's a **complete rethinking** of how AI-powered labeling should work:

1. **Democratizes Advanced Techniques**
   - Active learning ‚Üí Simple YAML config
   - Multi-model pipelines ‚Üí Easy orchestration
   - Quality validation ‚Üí Built-in agents

2. **Reduces Complexity**
   - SDK: 200 lines of code
   - ALO: 3 lines + YAML config

3. **Community-Driven Innovation**
   - Open source agents
   - Shareable workflows
   - Extensible architecture

4. **Best Practices Built-In**
   - Error handling
   - Retry logic
   - Status monitoring
   - Data validation

### **For the Community, By the Community**

- **MIT/Apache 2.0 License** - Use freely
- **CrewAI Framework** - Industry standard
- **Modular Design** - Easy contributions
- **Example Library** - Learn from others

---

## üîß When to Use What

### **Use SDK Directly When:**
- ‚úÖ Simple one-off operations
- ‚úÖ Custom integration requirements
- ‚úÖ Full control over every API call
- ‚úÖ Building custom tools

### **Use ALO When:**
- ‚úÖ Multi-step workflows
- ‚úÖ Production pipelines
- ‚úÖ Team collaboration
- ‚úÖ Reproducible processes
- ‚úÖ AI-powered labeling
- ‚úÖ Active learning loops
- ‚úÖ Quality assurance needed

---

## üí° The Vision

**ALO is not replacing the SDK - it's making it 10x more powerful and 100x easier to use.**

Just like:
- Django uses Python but makes web development easier
- Kubernetes uses Docker but makes orchestration easier
- ALO uses Labellerr SDK but makes labeling easier

---

## üåü Join the Movement

Help us make AI labeling accessible to everyone:

- ‚≠ê Star the repo: [github.com/1sarthakbhardawaj/labellerr-alo](https://github.com/1sarthakbhardawaj/labellerr-alo)
- ü§ù Contribute agents and workflows
- üí¨ Share your use cases
- üì¢ Spread the word

**Together, we're building the future of intelligent data labeling!** üöÄ
